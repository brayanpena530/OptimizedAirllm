{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> cache_utils installed\n"
     ]
    }
   ],
   "source": [
    "from airllm import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\braya\\OneDrive\\Documents\\Coding Enviroment\\OptimizedAirllm\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found_layers:{'model.embed_tokens.': True, 'model.layers.0.': True, 'model.layers.1.': True, 'model.layers.2.': True, 'model.layers.3.': True, 'model.layers.4.': True, 'model.layers.5.': True, 'model.layers.6.': True, 'model.layers.7.': True, 'model.layers.8.': True, 'model.layers.9.': True, 'model.layers.10.': True, 'model.layers.11.': True, 'model.layers.12.': True, 'model.layers.13.': True, 'model.layers.14.': True, 'model.layers.15.': True, 'model.layers.16.': True, 'model.layers.17.': True, 'model.layers.18.': True, 'model.layers.19.': True, 'model.layers.20.': True, 'model.layers.21.': True, 'model.layers.22.': True, 'model.layers.23.': True, 'model.layers.24.': True, 'model.layers.25.': True, 'model.layers.26.': True, 'model.layers.27.': True, 'model.layers.28.': True, 'model.layers.29.': True, 'model.layers.30.': True, 'model.layers.31.': True, 'model.norm.': True, 'lm_head.': True}\n",
      "saved layers already found in C:\\Users\\braya\\.cache\\huggingface\\hub\\models--meta-llama--Meta-Llama-3-8B-Instruct\\snapshots\\e1945c40cd546c78e41f1151f4db032b271faeaa\\splitted_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:02<00:00, 14.64it/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_Length = 128\n",
    "\n",
    "model = AutoModel.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", profiling_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device):   3%|▎         | 1/35 [00:00<00:24,  1.40it/s]c:\\Users\\braya\\OneDrive\\Documents\\Coding Enviroment\\OptimizedAirllm\\venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.0020296573638916016\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 48.3281\n",
      "total infer wall time(including all above plus gpu compute): 8.9327\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.002000570297241211\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 49.9062\n",
      "total infer wall time(including all above plus gpu compute): 8.7822\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.001997709274291992\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 51.3281\n",
      "total infer wall time(including all above plus gpu compute): 8.9153\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.0\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 51.3125\n",
      "total infer wall time(including all above plus gpu compute): 8.7290\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.0\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 49.6875\n",
      "total infer wall time(including all above plus gpu compute): 8.6797\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.000997304916381836\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 51.0625\n",
      "total infer wall time(including all above plus gpu compute): 9.0341\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.00099945068359375\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 50.7969\n",
      "total infer wall time(including all above plus gpu compute): 9.1639\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.0019998550415039062\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 50.3594\n",
      "total infer wall time(including all above plus gpu compute): 9.2828\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.0029990673065185547\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 51.3594\n",
      "total infer wall time(including all above plus gpu compute): 9.0701\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.0010001659393310547\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 51.7188\n",
      "total infer wall time(including all above plus gpu compute): 9.2238\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.0020041465759277344\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 50.7344\n",
      "total infer wall time(including all above plus gpu compute): 9.0362\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.002000570297241211\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 51.1406\n",
      "total infer wall time(including all above plus gpu compute): 8.9166\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.0019998550415039062\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 52.6719\n",
      "total infer wall time(including all above plus gpu compute): 9.2980\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.001996755599975586\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 53.8594\n",
      "total infer wall time(including all above plus gpu compute): 9.2988\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.0030019283294677734\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 50.9688\n",
      "total infer wall time(including all above plus gpu compute): 9.1941\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.0\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 50.7812\n",
      "total infer wall time(including all above plus gpu compute): 8.9776\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0010004043579101562\n",
      "total time for create_layer_from_state_dict: 0.005001544952392578\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 52.3594\n",
      "total infer wall time(including all above plus gpu compute): 9.2296\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.0009992122650146484\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 51.7812\n",
      "total infer wall time(including all above plus gpu compute): 9.2305\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.0019998550415039062\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 51.7656\n",
      "total infer wall time(including all above plus gpu compute): 9.0261\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:08<00:00,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0\n",
      "total time for compression_time: 0\n",
      "total time for pin_memory_to_trigger_load: 0\n",
      "total time for load_safe_tensor_cpu_wait: 0.0\n",
      "total time for create_layer_from_state_dict: 0.0029993057250976562\n",
      "total time for kick_off_load_cpu: 0.0\n",
      "total infer process time(including all above plus gpu compute): 51.2188\n",
      "total infer wall time(including all above plus gpu compute): 9.0378\n",
      "<|begin_of_text|>What is the capital of the United States? Washington D.C.\n",
      "What is the largest state in the United States by area? Alaska\n",
      "What is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = [\n",
    "        'What is the capital of the United States?',\n",
    "        #'I like',\n",
    "    ]\n",
    "\n",
    "input_tokens = model.tokenizer(input_text,\n",
    "    return_tensors=\"pt\", \n",
    "    return_attention_mask=False, \n",
    "    truncation=True, \n",
    "    max_length=MAX_Length, \n",
    "    padding=False)\n",
    "           \n",
    "generation_output = model.generate(\n",
    "    input_tokens['input_ids'].cuda(), \n",
    "    max_new_tokens=20,\n",
    "    use_cache=True,\n",
    "    return_dict_in_generate=True)\n",
    "\n",
    "output = model.tokenizer.decode(generation_output.sequences[0])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(self.running_device): 100%|██████████| 35/35 [00:12<00:00,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 0.13663434982299805\n",
      "total time for compression_time: 0.015625\n",
      "total time for pin_memory_to_trigger_load: 5.312329292297363\n",
      "total time for load_safe_tensor_cpu_wait: 2.967982292175293\n",
      "total time for create_layer_from_state_dict: 5.554761648178101\n",
      "total time for kick_off_load_cpu: 0.0010004043579101562\n",
      "total infer process time(including all above plus gpu compute): 93.6094\n",
      "total infer wall time(including all above plus gpu compute): 13.5372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "with cProfile.Profile() as pr:\n",
    "    generation_output = model.generate(\n",
    "    input_tokens['input_ids'].cuda(), \n",
    "    max_new_tokens=1,\n",
    "    use_cache=True,\n",
    "    return_dict_in_generate=True)\n",
    "\n",
    "stats = pstats.Stats(pr)\n",
    "stats.sort_stats(pstats.SortKey.TIME)\n",
    "stats.dump_stats(filename=\"profiling_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'A' object does not support the context manager protocol",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mb)\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteve\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'A' object does not support the context manager protocol"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class A:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, b):\n",
    "        print(\"hello \"+b)\n",
    "\n",
    "\n",
    "with A(), ThreadPoolExecutor as executor:\n",
    "    executor.submit(\"steve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
